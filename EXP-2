Exp 2 
Gautham Kuckian    
CSE(DS)_28
Back propogation in Deep Learning
Backpropagation is a crucial component of deep learning and forms the basis for training neural 
networks. It is an optimization algorithm used to adjust the parameters (weights and biases) of a 
neural network so that it can learn to make accurate predictions on a given task. The process 
involves two main steps: the forward pass and the backward pass.
Code:
import numpy as np 
class NeuralNetwork: 
 def __init__(self, input_size, hidden_size, output_size): 
 self.input_size = input_size 
 self.hidden_size = hidden_size 
 self.output_size = output_size 
 # Initialize weights and biases for the hidden layer and output layer 
 self.W1 = np.random.randn(hidden_size, input_size) 
 self.b1 = np.zeros((hidden_size, 1)) 
 self.W2 = np.random.randn(output_size, hidden_size) 
 self.b2 = np.zeros((output_size, 1)) 
 def sigmoid(self, x): 
 return 1 / (1 + np.exp(-x))
 def sigmoid_derivative(self, x): 
 return x * (1 - x)
 def forward(self, X): 
 # Forward pass 
 self.z1 = np.dot(self.W1, X) + self.b1 
 self.a1 = self.sigmoid(self.z1) 
 self.z2 = np.dot(self.W2, self.a1) + self.b2 
 self.a2 = self.sigmoid(self.z2) 
 return self.a2 
 def backward(self, X, y, learning_rate): 
 m = X.shape[1] 
 # Compute the gradients 
 dZ2 = self.a2 - y 
 dW2 = (1 / m) * np.dot(dZ2, self.a1.T) 
 db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True) 
 dZ1 = np.dot(self.W2.T, dZ2) * self.sigmoid_derivative(self.a1) 
 dW1 = (1 / m) * np.dot(dZ1, X.T) 
 db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True) 
 # Update weights and biases using gradients and learning rate 
 self.W2 -= learning_rate * dW2 
 self.b2 -= learning_rate * db2 
 self.W1 -= learning_rate * dW1 
 self.b1 -= learning_rate * db1 
 def train(self, X, y, epochs, learning_rate): 
 for epoch in range(epochs): 
 # Forward pass 
 predictions = self.forward(X) 
 # Compute the mean squared error loss 
 loss = np.mean((predictions - y) ** 2) 
 # Backward pass to update weights and biases 
 self.backward(X, y, learning_rate) 
 if epoch % 100 == 0: 
 print(f"Epoch {epoch}, Loss: {loss:.4f}") 
 def predict(self, X): 
 return self.forward(X) 
# Example usage: 
input_size = 2 
hidden_size = 4 
output_size = 1 
learning_rate = 0.5 
epochs = 10000 
# Generate some sample data 
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]]).T 
y = np.array([[0, 1, 1, 0]]) 
# Create the neural network 
nn = NeuralNetwork(input_size, hidden_size, output_size) 
# Train the neural network 
nn.train(X, y, epochs, learning_rate) 
# Make predictions 
predictions = nn.predict(X) 
print("Predictions:", predictions) 
